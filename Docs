Pre-bound JDBC Connection found! JpaTransactionManager does not support running within DataSourceTransactionManager if told to manage the DataSource itself. It is recommended to use a single JpaTransactionManager for all transactions on a single DataSource, no matter whether JPA or JDBC access.

@Service
public class EmailService {

    @Autowired
    private JavaMailSender mailSender;

    public void sendEmail(String to, String subject, String text) throws MessagingException {
        MimeMessage message = mailSender.createMimeMessage();
        MimeMessageHelper helper = new MimeMessageHelper(message, true);

        helper.setTo(to);
        helper.setSubject(subject);
        helper.setText(text, true);

        mailSender.send(message);
    }
}


spring.mail.host=smtp.example.com
spring.mail.port=587
spring.mail.username=your_username
spring.mail.password=your_password
spring.mail.properties.mail.smtp.auth=true
spring.mail.properties.mail.smtp.starttls.enable=true




-------------------------------------------------
I believe you have configured the incorrect URL, and upon closer inspection, we have noticed that the URL you are referring to is entirely out of date. I kindly ask you to configure the proper URL, as I believe Vikas shared it with you a long time ago. Please refer to the email from Vikas that is attached.
Then you should see an accurate record if your browser is definitely compatible. I can see my TP ID on the page, but the segam jar component cannot be loaded on my computer. 

In order to find out how you are using an application, I am setting up a call.
Nevertheless, we haven't changed anything about the current context path.

Please involve someone who has previously tested this application as we have not made any modifications to the current application context path.


I am scheduling a call to understand how you are accessing an application.
However we have not made any changes in the existing context path.

we have not make any changes in the existing application context path,please involve someone who already done the testing of this application.

We received an incorrect password. The password is not loading for us.

We can decrypt the previous password, which was about sixty characters long, and send it to the calling channel.

We are unable to understand the new password, though, because it is only 28 characters long.

The password we received is invalid we are unable to load the password.

The old password is close to 60 char long and old password we are able to decrypt and pass to calling channel

However  the new password is only 28 character long so we are unable to parse it.


I would like to know who tested the application previously. Please contact Vikas to find out how they are conducting the testing. Please note that you cannot access this URL directly in a browser. I have not altered the deployed application context route.

Nevertheless, you can test the new modifications after confirming the current flow in an alternative context path, as specified by Vikas in the email below.

<Valve className="org.apache.catalina.valves.RemoteAddrValve"
       allow="127\.\d+\.\d+\.\d+|::1|192\.168\.1\.100"/>

Following the adjustment, we are supposed to promote manufacturing.
1.Log4j version upgrade
2. Database suppresses request/response
3. As previously indicated, we need to deploy services in UAT and align the business to do a round of sanity checks.

Please find some notes in the attached spreadsheet.


When we attempt to connect from the service, we receive the following exception. 
Would you kindly see whether there is a problem with UAT DB?

I'd want to highlight a few points: 1. Please check the DATETIME value for the record you've shown, which is 25-MAR-24. 2.And, according to the business logic, we are only picking up records from the last 10 days, which is outside of the condition, which is why the batch job does not choose them.

3.As a result, the batch did not choose records that we had not stamped into the database.
4.and in all of the records you provided below, the datetime is before May.



There are few points which I want to highlight 
1.The record which you have shown in that can you please check what is DATETIME value it is 25-MAR-24 
2.and as per the business logic we are only picking up past 10 days record so that is beyond the condition that is the reason batch job not select those record.
3.and hence batch didnâ€™t pick those record we have not stamp into the database.
4.and whatever record you have given below in all the record I can see datetime is before may.

import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.Bean;
import org.springframework.http.HttpEntity;
import org.springframework.http.HttpHeaders;
import org.springframework.http.HttpMethod;
import org.springframework.http.ResponseEntity;
import org.springframework.util.ResourceUtils;
import org.springframework.web.client.RestTemplate;

import javax.net.ssl.*;
import java.io.FileInputStream;
import java.security.KeyStore;

@SpringBootApplication
public class Application {

    @Value("${your.jks.password}")
    private String jksPassword;

    @Value("${your.jks.path}")
    private String jksPath;

    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }

    @Bean
    public RestTemplate restTemplate() throws Exception {
        // Load the JKS file
        KeyStore keyStore = KeyStore.getInstance("JKS");
        FileInputStream fis = new FileInputStream(ResourceUtils.getFile(jksPath));
        keyStore.load(fis, jksPassword.toCharArray());

        // Create and configure SSLContext
        SSLContext sslContext = SSLContext.getInstance("TLS");
        KeyManagerFactory keyManagerFactory = KeyManagerFactory.getInstance(KeyManagerFactory.getDefaultAlgorithm());
        keyManagerFactory.init(keyStore, jksPassword.toCharArray());
        TrustManagerFactory trustManagerFactory = TrustManagerFactory.getInstance(TrustManagerFactory.getDefaultAlgorithm());
        trustManagerFactory.init(keyStore);
        sslContext.init(keyManagerFactory.getKeyManagers(), trustManagerFactory.getTrustManagers(), null);

        // Configure RestTemplate to use SSLContext
        HttpsURLConnection.setDefaultSSLSocketFactory(sslContext.getSocketFactory());

        return new RestTemplate();
    }

    public void callHttpPost() {
        // Prepare request payload
        HttpHeaders headers = new HttpHeaders();
        // Add headers if needed
        // headers.add("Content-Type", "application/json");

        // Example payload
        String requestBody = "{\"key\": \"value\"}";

        // Execute POST request
        String url = "https://example.com/your_endpoint";
        HttpEntity<String> requestEntity = new HttpEntity<>(requestBody, headers);
        ResponseEntity<String> response = restTemplate().exchange(url, HttpMethod.POST, requestEntity, String.class);

        // Handle response
        System.out.println("Response status: " + response.getStatusCode());
        System.out.println("Response body: " + response.getBody());
    }
}



import org.springframework.http.HttpEntity;
import org.springframework.http.HttpHeaders;
import org.springframework.http.HttpMethod;
import org.springframework.http.ResponseEntity;
import org.springframework.util.ResourceUtils;
import org.springframework.web.client.RestTemplate;

import java.io.File;
import java.io.FileInputStream;
import java.security.KeyStore;
import javax.net.ssl.KeyManagerFactory;
import javax.net.ssl.SSLContext;
import javax.net.ssl.TrustManagerFactory;

public class Main {
    public static void main(String[] args) throws Exception {
        // Load the JKS file
        File jksFile = ResourceUtils.getFile("classpath:path_to_your_jks_file.jks");
        FileInputStream fis = new FileInputStream(jksFile);

        KeyStore keyStore = KeyStore.getInstance("JKS");
        keyStore.load(fis, "your_jks_password".toCharArray());

        // Create a KeyManagerFactory and initialize it with the KeyStore
        KeyManagerFactory keyManagerFactory = KeyManagerFactory.getInstance(KeyManagerFactory.getDefaultAlgorithm());
        keyManagerFactory.init(keyStore, "your_jks_password".toCharArray());

        // Create a TrustManagerFactory and initialize it with the KeyStore
        TrustManagerFactory trustManagerFactory = TrustManagerFactory.getInstance(TrustManagerFactory.getDefaultAlgorithm());
        trustManagerFactory.init(keyStore);

        // Create SSLContext
        SSLContext sslContext = SSLContext.getInstance("TLS");
        sslContext.init(keyManagerFactory.getKeyManagers(), trustManagerFactory.getTrustManagers(), null);

        // Configure RestTemplate to use the SSLContext
        RestTemplate restTemplate = new RestTemplate();
        restTemplate.setRequestFactory(new HttpsClientRequestFactory(sslContext));

        // Prepare request payload
        HttpHeaders headers = new HttpHeaders();
        // Add headers if needed
        // headers.add("Content-Type", "application/json");

        // Example payload
        String requestBody = "{\"key\": \"value\"}";

        // Execute POST request
        String url = "https://example.com/your_endpoint";
        HttpEntity<String> requestEntity = new HttpEntity<>(requestBody, headers);
        ResponseEntity<String> response = restTemplate.exchange(url, HttpMethod.POST, requestEntity, String.class);

        // Handle response
        System.out.println("Response status: " + response.getStatusCode());
        System.out.println("Response body: " + response.getBody());
    }
}



SELECT *
FROM (
    SELECT *
    FROM your_table
    WHERE your_conditions
) subq
WHERE ROWNUM <= 100;

CREATE OR REPLACE PROCEDURE update_in_batches AS
    CURSOR cur_data IS
        SELECT your_primary_key, your_column
        FROM your_table
        WHERE your_conditions; -- Conditions to filter the rows you want to update
    TYPE id_list IS TABLE OF your_table.primary_key%TYPE INDEX BY PLS_INTEGER;
    v_ids id_list;
BEGIN
    OPEN cur_data;
    LOOP
        FETCH cur_data BULK COLLECT INTO v_ids LIMIT 1000; -- Adjust the batch size as needed
        EXIT WHEN v_ids.COUNT = 0;
        
        FOR i IN 1..v_ids.COUNT LOOP
            UPDATE your_table
            SET your_column = (
                -- Your subquery here to fetch the result
                SELECT subquery_result_column
                FROM (
                    SELECT your_column, subquery_result_column
                    FROM your_table
                    WHERE your_conditions -- Conditions to filter the rows you want to update
                ) subq
                WHERE subq.your_column = v_ids(i)
            )
            WHERE your_primary_key = v_ids(i);
        END LOOP;

        COMMIT;
    END LOOP;
    CLOSE cur_data;
END update_in_batches;
/

CREATE OR REPLACE PROCEDURE update_in_batches AS
    CURSOR cur_data IS
        -- Your inner query here to fetch the records
        SELECT your_primary_key, your_column
        FROM your_table
        WHERE your_conditions; -- Conditions to filter the rows you want to update
    TYPE id_list IS TABLE OF your_table.primary_key%TYPE INDEX BY PLS_INTEGER;
    v_ids id_list;
BEGIN
    OPEN cur_data;
    LOOP
        FETCH cur_data BULK COLLECT INTO v_ids LIMIT 1000; -- Adjust the batch size as needed
        EXIT WHEN v_ids.COUNT = 0;
        
        FOR i IN 1..v_ids.COUNT LOOP
            -- Your update statement here
            UPDATE your_table
            SET your_column = (
                -- Your subquery here to fetch the result
                SELECT subquery_result_column
                FROM (
                    SELECT your_column, subquery_result_column
                    FROM your_table
                    WHERE your_conditions -- Conditions to filter the rows you want to update
                ) subq
                WHERE subq.your_column = v_ids(i)
            )
            WHERE your_primary_key = v_ids(i);
        END LOOP;

        COMMIT;
    END LOOP;
    CLOSE cur_data;
END update_in_batches;
/


DECLARE
    BATCH_SIZE NUMBER := 1000; -- Adjust the batch size as needed
    TOTAL_RECORDS NUMBER;
BEGIN
    SELECT COUNT(*)
    INTO TOTAL_RECORDS
    FROM your_table
    WHERE your_conditions; -- Conditions to filter the rows you want to update

    FOR i IN 1..CEIL(TOTAL_RECORDS / BATCH_SIZE) LOOP
        UPDATE your_table t
        SET t.your_column = (
            SELECT subquery_result_column
            FROM (
                -- Your subquery here to fetch the result
                SELECT your_column, subquery_result_column
                FROM your_table
                WHERE your_conditions -- Conditions to filter the rows you want to update
            ) subq
            WHERE subq.your_column = t.your_column
            AND ROWNUM <= BATCH_SIZE * i
            AND ROWNUM > BATCH_SIZE * (i - 1)
        );

        COMMIT;
    END LOOP;
END;
/



UPDATE your_table t
SET t.your_column = (
    SELECT subquery_result_column
    FROM (
        -- Your subquery here to fetch the result
        SELECT your_column, subquery_result_column
        FROM your_table
        WHERE your_conditions -- Conditions to filter the rows you want to update
    ) subq
    WHERE subq.your_column = t.your_column
);
WITH subquery_result AS (
    SELECT id, SUM(value) AS total_value
    FROM your_table
    GROUP BY id
)
UPDATE your_table t
SET t.total_value = (
    SELECT sr.total_value
    FROM subquery_result sr
    WHERE sr.id = t.id
);



import org.springframework.http.client.ClientHttpRequestFactory;
import org.springframework.http.client.HttpComponentsClientHttpRequestFactory;
import org.apache.http.client.HttpClient;
import org.apache.http.conn.ssl.NoopHostnameVerifier;
import org.apache.http.impl.client.HttpClients;
import org.apache.http.ssl.SSLContextBuilder;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.web.client.RestTemplate;

import javax.net.ssl.SSLContext;

@Configuration
public class RestTemplateConfig {

    @Value("${your.jks.file.path}")
    private String jksFilePath;

    @Value("${your.jks.password}")
    private String jksPassword;

    @Bean
    public RestTemplate restTemplate() throws Exception {
        return new RestTemplate(clientHttpRequestFactory());
    }

    private ClientHttpRequestFactory clientHttpRequestFactory() throws Exception {
        SSLContext sslContext = SSLContextBuilder
                .create()
                .loadKeyMaterial(new File(jksFilePath), jksPassword.toCharArray(), jksPassword.toCharArray())
                .build();

        HttpClient client = HttpClients.custom()
                .setSSLContext(sslContext)
                .setSSLHostnameVerifier(NoopHostnameVerifier.INSTANCE)
                .build();

        return new HttpComponentsClientHttpRequestFactory(client);
    }
}


SELECT COUNT(*)
FROM 
  service_api.SERVICE_API_REQ_RES_V2 AS SAL 
  INNER JOIN service_api.TBL_ESB_SERVICELOG AS ESB ON SAL.INTERACTION_ID = ESB.INTERACTION_ID
WHERE
  SAL.SERVICE_NAME = '/apipaymentsV3/v3/corporate/fund-transfer/adhoc' 
  AND SAL.STATUS = 'FAILURE' 
  AND SAL.REQTIMESTAMP >= DATE_SUB(NOW(), INTERVAL 60 HOUR) 
  AND SAL.REQTIMESTAMP <= ESB.REQ_DATETIME;


Indexing: Ensure that appropriate indexes are in place on the columns used in the WHERE clause (SERVICE_NAME, STATUS, REQTIMESTAMP, and INTERACTION_ID), as well as on the join column (INTERACTION_ID). Indexes can significantly speed up the lookup process.
Use INNER JOIN: If the join condition guarantees that there will always be a corresponding record in TBL_ESB_SERVICELOG, consider using INNER JOIN instead of LEFT JOIN. INNER JOIN tends to be faster because it only returns matching records.
Avoid ORDER BY: Since you're only interested in the count of rows and not the actual data, removing the ORDER BY clause can improve performance. Sorting can be costly, especially for large result sets.
Avoid Functions in WHERE Clause: Using functions like date_sub() on columns (REQTIMESTAMP) can prevent the query optimizer from utilizing indexes efficiently. Instead, consider rewriting the condition without the function, if possible.

SELECT COUNT(*)  
FROM 
  service_api.SERVICE_API_REQ_RES_V2 as SAL 
  LEFT JOIN service_api.TBL_ESB_SERVICELOG as ESB on SAL.INTERACTION_ID = ESB.INTERACTION_ID
WHERE
  SAL.SERVICE_NAME ='/apipaymentsV3/v3/corporate/fund-transfer/adhoc' 
  AND SAL.STATUS = 'FAILURE' 
  AND SAL.REQTIMESTAMP >= date_sub(now(), INTERVAL 60 hour) 
  AND SAL.REQTIMESTAMP <= ESB.REQ_DATETIME
  -- AND ESB.RES_DATETIME <= SAL.RESPTIMESTAMP
ORDER BY 
  SAL.REQID desc;

Partitioning can enhance query performance by allowing the database engine to limit the scan to specific partitions rather than the entire table. This is especially beneficial for tables with large volumes of data, as it reduces the number of rows that need to be scanned for a particular query.

CREATE TABLE your_table_name (
    id INT,
    status VARCHAR(50),
    -- other columns
)
PARTITION BY RANGE COLUMNS(status) (
    PARTITION p_success VALUES LESS THAN ('success'),
    PARTITION p_failure VALUES LESS THAN ('failure'),
    PARTITION p_pending VALUES LESS THAN ('pending'),
    PARTITION p_other VALUES LESS THAN (MAXVALUE)
);

SELECT * FROM INFORMATION_SCHEMA.PROCESSLIST;

This might involve adding or modifying indexes,
 rewriting the queries to be more efficient, or restructuring the database schema.


Deleting older records reduces the overall volume of data stored in the system. 
This can lead to faster query execution times since there's less data to scan through-->

we can use timing event to delete record which are older then 1 or 2 month and that will run once in a month.


create indexing on the table column in which we are hitting the queries(status)

CREATE TRIGGER dump_on_failure
AFTER INSERT ON your_table_name
FOR EACH ROW
BEGIN
    -- Check if the status column has been updated to 'failure'
    IF NEW.status = 'failure' THEN
        -- Insert the updated row into another table for data dumping
        INSERT INTO another_table_name (column1, column2, ..., status)
        VALUES (NEW.column1, NEW.column2, ..., NEW.status);
    END IF;
END;

We were using the http url before, but you told us to switch to the https one. The certificate we received is in a different format, so we ask that you check with the middleware team. They will provide you with the JKS file and password, and we will check to see if we can connect to the UAT firewall. We need to raise a firewall request because we are unable to connect from any environment.
We have already upgraded the log4j version in a few services and made a Confluence page available for production use.



To exclude the controller advice for the health check URL, you can use a custom `HandlerInterceptor`. Here's how you can do it:

```java
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.autoconfigure.web.servlet.WebMvcRegistrations;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.web.servlet.HandlerInterceptor;
import org.springframework.web.servlet.config.annotation.InterceptorRegistry;
import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;
import org.springframework.web.servlet.mvc.method.annotation.ExceptionHandlerExceptionResolver;

import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;

@Configuration
public class CustomWebMvcRegistrations {

    @Autowired
    private HealthEndpointProperties healthEndpointProperties;

    @Bean
    public WebMvcRegistrations webMvcRegistrations() {
        return new WebMvcRegistrations() {
            @Override
            public ExceptionHandlerExceptionResolver getExceptionHandlerExceptionResolver() {
                return new ExceptionHandlerExceptionResolver() {
                    @Override
                    protected boolean shouldApplyTo(HttpServletRequest request, Object handler) {
                        // Exclude health check actuator endpoints from invoking controller advice
                        String requestUri = request.getRequestURI();
                        String healthEndpointPath = healthEndpointProperties.getPath();
                        return !requestUri.startsWith(healthEndpointPath);
                    }
                };
            }
        };
    }

    @Bean
    public WebMvcConfigurer webMvcConfigurer() {
        return new WebMvcConfigurer() {
            @Override
            public void addInterceptors(InterceptorRegistry registry) {
                registry.addInterceptor(new HealthCheckInterceptor()).addPathPatterns(healthEndpointProperties.getPath());
            }
        };
    }

    private static class HealthCheckInterceptor implements HandlerInterceptor {
        @Override
        public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {
            // Exclude controller advice for health check URL
            return true;
        }
    }
}
```

In this configuration:

- The `CustomWebMvcRegistrations` class excludes the controller advice for the health check URL and sets up a custom `HandlerInterceptor`.
- The `HandlerInterceptor` named `HealthCheckInterceptor` is created to intercept requests to the health check URL. In the `preHandle` method, you can add logic to exclude controller advice for this URL.
- The `WebMvcConfigurer` is used to register the `HealthCheckInterceptor` with the appropriate URL pattern using `addPathPatterns`.
- The health endpoint properties are injected using `@Autowired`. Ensure you have `HealthEndpointProperties` properly configured in your project.

Adjust the code according to your specific controller advice and health endpoint configuration.



To achieve this, you'll want to exclude the controller advice from being applied to the health check actuator endpoints and also set all health check properties. Here's how you can do it:

```java
import org.springframework.boot.actuate.autoconfigure.endpoint.web.WebEndpointProperties;
import org.springframework.boot.actuate.autoconfigure.web.server.ManagementServerProperties;
import org.springframework.boot.actuate.health.HealthEndpointProperties;
import org.springframework.boot.autoconfigure.web.servlet.WebMvcRegistrations;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.web.servlet.mvc.method.annotation.ExceptionHandlerExceptionResolver;

@Configuration
public class CustomWebMvcRegistrations {

    private final ManagementServerProperties managementServerProperties;
    private final WebEndpointProperties webEndpointProperties;
    private final HealthEndpointProperties healthEndpointProperties;

    public CustomWebMvcRegistrations(ManagementServerProperties managementServerProperties,
                                      WebEndpointProperties webEndpointProperties,
                                      HealthEndpointProperties healthEndpointProperties) {
        this.managementServerProperties = managementServerProperties;
        this.webEndpointProperties = webEndpointProperties;
        this.healthEndpointProperties = healthEndpointProperties;
    }

    @Bean
    public WebMvcRegistrations webMvcRegistrations() {
        return new WebMvcRegistrations() {
            @Override
            public ExceptionHandlerExceptionResolver getExceptionHandlerExceptionResolver() {
                return new ExceptionHandlerExceptionResolver() {
                    @Override
                    protected boolean shouldApplyTo(HttpServletRequest request, Object handler) {
                        // Exclude health check actuator endpoints from invoking controller advice
                        String requestUri = request.getRequestURI();
                        String healthEndpointPath = managementServerProperties.getBasePath()
                                + webEndpointProperties.getBasePath() + healthEndpointProperties.getPath();
                        return !requestUri.startsWith(healthEndpointPath);
                    }
                };
            }
        };
    }
}
```

In this configuration:

- The `CustomWebMvcRegistrations` class is responsible for excluding controller advice for health check actuator endpoints and setting all health check properties.
- We inject `ManagementServerProperties`, `WebEndpointProperties`, and `HealthEndpointProperties` to access the properties related to actuator endpoints.
- The `shouldApplyTo` method checks if the request URI starts with the health endpoint path and excludes applying controller advice if it does. Adjust the condition according to your actuator endpoint structure.
- Ensure you have the necessary dependencies and proper configuration to enable auto-injection of these properties into the constructor of `CustomWebMvcRegistrations`.



To exclude the controller advice from being invoked for health check actuator endpoints, you can create a custom configuration to selectively disable it. Here's how you can achieve this:

```java
import org.springframework.boot.autoconfigure.web.servlet.WebMvcRegistrations;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.web.servlet.mvc.method.annotation.ExceptionHandlerExceptionResolver;

@Configuration
public class CustomWebMvcRegistrations {

    @Bean
    public WebMvcRegistrations webMvcRegistrations() {
        return new WebMvcRegistrations() {
            @Override
            public ExceptionHandlerExceptionResolver getExceptionHandlerExceptionResolver() {
                return new ExceptionHandlerExceptionResolver() {
                    @Override
                    protected boolean shouldApplyTo(HttpServletRequest request, Object handler) {
                        // Exclude health check actuator endpoints from invoking controller advice
                        String requestUri = request.getRequestURI();
                        return !requestUri.contains("/actuator/health");
                    }
                };
            }
        };
    }
}
```

In this configuration, a custom `WebMvcRegistrations` bean is created to override the default `ExceptionHandlerExceptionResolver`. The `shouldApplyTo` method is then customized to exclude the health check actuator endpoints from invoking controller advice by checking if the request URI contains `/actuator/health`.

Make sure to adjust the condition according to your actual actuator endpoint URLs if they differ from the standard `/actuator/health`.



DELIMITER //

CREATE PROCEDURE delete_old_data()
BEGIN
    DECLARE cutoff_date DATE;
    
    -- Calculate the cutoff date (6 months ago)
    SET cutoff_date = DATE_SUB(NOW(), INTERVAL 6 MONTH);
    
    -- Delete records older than the cutoff date
    DELETE FROM your_table_name WHERE timestamp_column < cutoff_date;
END//

DELIMITER ;


DELIMITER //

CREATE PROCEDURE delete_records_in_batches(IN table_name VARCHAR(255), IN timestamp_column VARCHAR(255), IN batch_size INT)
BEGIN
    DECLARE done BOOLEAN DEFAULT FALSE;
    DECLARE total_rows INT;
    DECLARE rows_deleted INT;

    -- Construct the SQL query dynamically
    SET @sql = CONCAT('SELECT COUNT(*) INTO @total_rows FROM ', table_name, ' WHERE ', timestamp_column, ' < NOW();');
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;

    -- Loop until all rows are deleted
    WHILE NOT done DO
        -- Construct the SQL query dynamically
        SET @sql = CONCAT('DELETE FROM ', table_name, ' WHERE ', timestamp_column, ' < NOW() LIMIT ', batch_size, ';');
        PREPARE stmt FROM @sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;

        -- Check how many rows were deleted in the last batch
        SET rows_deleted = ROW_COUNT();

        -- If fewer rows were deleted than the batch size, we're done
        IF rows_deleted < batch_size THEN
            SET done = TRUE;
        END IF;
    END WHILE;
END//

DELIMITER ;


DELIMITER //

CREATE PROCEDURE delete_data_in_batches()
BEGIN
    DECLARE done BOOLEAN DEFAULT FALSE;
    DECLARE batch_size INT DEFAULT 1000; -- Adjust batch size as needed
    DECLARE total_rows INT;
    DECLARE rows_deleted INT;

    -- Create an index on the datetime column if not already exists
    CREATE INDEX idx_datetime ON your_table_name(datetime_column);

    -- Get the total number of rows to be deleted
    SELECT COUNT(*) INTO total_rows FROM your_table_name WHERE datetime_column < 'desired_datetime';

    -- Loop until all rows are deleted
    WHILE NOT done DO
        -- Delete data in batches
        DELETE FROM your_table_name WHERE datetime_column < 'desired_datetime' LIMIT batch_size;
        SET rows_deleted = ROW_COUNT();
        IF rows_deleted < batch_size THEN
            SET done = TRUE;
        END IF;
    END WHILE;
END//

DELIMITER ;





In a Spring Boot application, you can use the `java.nio.file.Files` class to delete particular files. Here's an example of how you can delete specific files based on their names:

```java
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;

public class FileDeletionExample {

    public static void main(String[] args) {
        String directoryPath = "path/to/your/directory";
        String[] filesToDelete = {"file1.txt", "file2.txt", "file3.txt"};
        deleteSpecificFiles(directoryPath, filesToDelete);
    }

    public static void deleteSpecificFiles(String directoryPath, String[] filesToDelete) {
        for (String fileName : filesToDelete) {
            Path filePath = Paths.get(directoryPath, fileName);
            try {
                Files.delete(filePath);
                System.out.println("Deleted file: " + fileName);
            } catch (IOException e) {
                System.err.println("Failed to delete file: " + fileName);
            }
        }
    }
}
```

Replace `"path/to/your/directory"` with the actual path of the directory containing the files you want to delete, and `"file1.txt"`, `"file2.txt"`, `"file3.txt"`, etc., with the names of the files you want to delete. This approach deletes only the specified files within the directory.



CREATE TABLE documents (
    document_id INT PRIMARY KEY AUTO_INCREMENT,
    document_name VARCHAR(255),
    document_type VARCHAR(50),
    uploader_user_id INT, -- Assuming this is a foreign key
    upload_timestamp TIMESTAMP,
    file_path_url VARCHAR(255),
    edms_location_id INT,
    upload_status ENUM('pending', 'completed', 'failed'),
    file_size INT,
    file_format_extension VARCHAR(10),
    -- Other columns...
);


Document ID
Document Name:
Document Type:
Upload Timestamp:
File Path/URL:

EDMS Location/ID:
Upload Status:
A status indicator (e.g., pending, completed, failed) to track the success or failure of the upload process.
Original NAS Location:

Store information about the original location of the document on the NAS before being transferred to the EDMS.


INSERT INTO employees (id, name, salary)
VALUES
  (1, 'John Doe', 50000),
  (2, 'Jane Smith', 60000),
  (3, 'Bob Johnson', 55000);

java.sql.SQLException: Can not issue data manipulation statements with executeQuery()

@Transactional
    @Modifying
    @Query("UPDATE YourEntity e SET e.column1 = :newColumn1, e.column2 = :newColumn2 WHERE e.id = :id")
    void updateColumns(@Param("id") Long id, @Param("newColumn1") String newColumn1, @Param("newColumn2") String newColumn2);

DELIMITER //

CREATE PROCEDURE YourInsertProcedure()
BEGIN
    -- Your Common Table Expressions (CTEs)
    WITH CTE1 AS (
        SELECT CHANNEL_NAME, appliance_REQUEST_URL, COUNT(appliance_INTERACTION_ID) AS TOTAL_CALL
        FROM CombinedData
        GROUP BY CHANNEL_NAME, appliance_REQUEST_URL
    ),
    CTE2 AS (
        -- Another CTE if needed
        -- ...
    )

    -- Insert data into the destination table from the CTEs
    INSERT INTO EFR_STATS (CHANNEL, REQUEST_URL, TOTAL_API_CALL)
    SELECT CHANNEL_NAME, appliance_REQUEST_URL, TOTAL_CALL
    FROM CTE1;

    -- Your logic after the insert statement if needed
END //

DELIMITER ;


INSERT INTO EFR_STATS(CHANNEL,REQUEST_URL,TOTAL_API_CALL) SELECT CHANNEL_NAME,appliance_REQUEST_URL,count(appliance_INTERACTION_ID) AS TOTAL_CALL FROM CombinedData group by CHANNEL_NAME,appliance_REQUEST_URL;

DELIMITER //

CREATE PROCEDURE CheckItemsExistence(
    IN p_item_list TEXT
)
BEGIN
    -- Declare a variable to store the result
    DECLARE result VARCHAR(255);

    -- Your logic to check if any item in the list exists in the table column
    SELECT CASE
        WHEN EXISTS (
            SELECT 1
            FROM your_table
            WHERE FIND_IN_SET(your_column, p_item_list)
        ) THEN 'Some items exist in the column'
        ELSE 'No items found'
    END INTO result;

    -- You can now use or return the result as needed
    SELECT result AS Result;
END //

DELIMITER ;
CALL CheckItemsExistence('item1,item2,item3');


DELIMITER //

CREATE PROCEDURE OuterProcedure()
BEGIN
    DECLARE exit_handler BOOLEAN DEFAULT FALSE;

    -- Declare a continue handler to catch any SQL exceptions
    DECLARE CONTINUE HANDLER FOR SQLEXCEPTION
    BEGIN
        -- Set the exit_handler to TRUE to indicate an exception
        SET exit_handler = TRUE;
    END;

    -- Your logic for the outer procedure
    -- ...

    -- Call the inner procedure, and if it fails, the handler will be triggered,
    -- but the outer procedure will continue executing
    CALL InnerProcedure();

    -- Check if an exception occurred in the inner procedure
    IF exit_handler THEN
        -- Handle the exception if needed
        -- You can log the error, set a status variable, etc.
        -- For example:
        -- SET status_variable = 'Failure';
    END IF;

    -- Continue with the rest of the outer procedure logic
    -- ...

END //

DELIMITER ;



DELIMITER //

CREATE PROCEDURE YourProcedure()
BEGIN
    -- Declare variables for job details
    DECLARE job_name VARCHAR(255);
    DECLARE start_time DATETIME;
    DECLARE end_time DATETIME;
    DECLARE job_status ENUM('RUNNING', 'SUCCESS', 'FAILURE');
    DECLARE job_details TEXT;

    -- Set job details
    SET job_name = 'YourJobName'; -- Replace with your job name
    SET start_time = NOW();

    -- Your logic for the stored procedure goes here
    -- ...

    -- Set end time based on the success or failure of the procedure
    IF YourProcedureLogicIsSuccessful() THEN
        SET end_time = NOW();
        SET job_status = 'SUCCESS';
        SET job_details = 'Procedure executed successfully.';
    ELSE
        SET end_time = NOW();
        SET job_status = 'FAILURE';
        SET job_details = 'Procedure execution failed.';
    END IF;

    -- Insert job details into the job table
    INSERT INTO job (job_name, start_time, end_time, status, details)
    VALUES (job_name, start_time, end_time, job_status, job_details);
END //

DELIMITER ;



CREATE TABLE job_execution (
    job_id INT AUTO_INCREMENT PRIMARY KEY,
    job_name VARCHAR(255) NOT NULL,
    start_time DATETIME NOT NULL,
    end_time DATETIME,
    status ENUM('RUNNING', 'SUCCESS', 'FAILURE') DEFAULT 'RUNNING',
    details TEXT,
    -- Add any additional columns as needed
    INDEX idx_start_time (start_time),
    INDEX idx_end_time (end_time),
    INDEX idx_status (status)
);


DATE_SUB(CURDATE(), INTERVAL 1 DAY)

DATE_SUB(CURDATE(), INTERVAL 1 SECOND)

DELIMITER //

CREATE PROCEDURE FetchDayBeforeRecords()
BEGIN
    DECLARE yesterday_start DATETIME;
    DECLARE yesterday_end DATETIME;

    -- Calculate start of yesterday (00:00:00)
    SET yesterday_start = DATE_SUB(NOW(), INTERVAL 1 DAY);

    -- Calculate end of yesterday (23:59:59)
    SET yesterday_end = DATE_SUB(NOW(), INTERVAL 1 SECOND);

    -- Now you can use the yesterday_start and yesterday_end variables in your query
    SELECT *
    FROM your_table
    WHERE your_datetime_column BETWEEN yesterday_start AND yesterday_end;

END //

DELIMITER ;


DELIMITER //

CREATE PROCEDURE FetchRecordsForDateRange(IN input_date DATE)
BEGIN
    DECLARE start_date DATE;
    DECLARE end_date DATE;

    -- Calculate start date (input_date - 1 day)
    SET start_date = DATE_SUB(input_date, INTERVAL 1 DAY);

    -- Set end date to the input_date
    SET end_date = input_date;

    -- Now you can use the start_date and end_date variables in your query
    SELECT *
    FROM your_table
    WHERE your_date_column BETWEEN start_date AND end_date;

END //

DELIMITER ;



SELECT *
FROM your_table
WHERE your_date_column < CURDATE();

SELECT *
FROM your_table
WHERE your_date_column = DATE_SUB(CURDATE(), INTERVAL 1 DAY);

DECLARE result_date DATE;

    -- Subtract 1 day from the input date
    SET result_date = DATE_SUB(input_date, INTERVAL 1 DAY);

-- Add the last partition with MAXVALUE
    SET @sql = CONCAT(@sql, 'PARTITION p', current_year, ' VALUES LESS THAN MAXVALUE);');

DELIMITER //

CREATE PROCEDURE your_procedure()
BEGIN
    -- Declare variables
    DECLARE combinedyearmonth VARCHAR(7);

    -- Set the combined year and month value
    SET combinedyearmonth = CONCAT(YEAR(NOW()), '-', LPAD(MONTH(NOW()), 2, '0'));

    -- Create a temporary table with the combined year and month value
    CREATE TEMPORARY TABLE temp_table AS (
        SELECT 
            your_table.*,
            combinedyearmonth AS year_month_combined
        FROM 
            your_table
    );

    -- Use the temporary table in your further logic
    SELECT * FROM temp_table;

    -- Drop the temporary table when done
    DROP TEMPORARY TABLE IF EXISTS temp_table;
END //

DELIMITER ;



ALTER TABLE your_table
ADD PARTITION (
    PARTITION p_before_2024 VALUES LESS THAN (TO_DAYS('2024-01-01'))
);
Add Monthly Partitions Starting from 2024:

DELIMITER //

CREATE PROCEDURE AddMonthlyPartitions()
BEGIN
    DECLARE i INT DEFAULT 1;

    WHILE i <= 12 DO
        SET @sql = CONCAT('ALTER TABLE your_table ADD PARTITION (PARTITION p_', DATE_FORMAT('2024-01-01', '%Y%m'), 
                          ' VALUES LESS THAN (', TO_DAYS('2024-01-01' + INTERVAL i MONTH), '))');
        PREPARE stmt FROM @sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;

        SET i = i + 1;
    END WHILE;
END //

DELIMITER ;

-- Execute the stored procedure to add partitions for the next 12 months starting from 2024
CALL AddMonthlyPartitions();



ALTER TABLE your_table
ADD PARTITION (
    PARTITION p_next_month VALUES LESS THAN (TO_DAYS(NOW()) + 32)
);

SELECT * FROM your_table
WHERE
    YEAR(date_column) = your_year
    AND MONTH(date_column) = your_month;


CREATE TABLE your_table (
    id INT,
    date_column DATE,
    -- other columns...
) PARTITION BY RANGE COLUMNS (YEAR(date_column), MONTH(date_column)) (
    PARTITION p0 VALUES LESS THAN (2000, 1),
    PARTITION p1 VALUES LESS THAN (2000, 2),
    PARTITION p2 VALUES LESS THAN (2000, 3),
    -- Continue creating partitions for each year and month combination
    PARTITION p_max VALUES LESS THAN (MAXVALUE, MAXVALUE)
);



CREATE TABLE your_table (
   -- columns...
) PARTITION BY RANGE (YEAR(date_column)) (
   PARTITION p0 VALUES LESS THAN (1991),
   PARTITION p1 VALUES LESS THAN (1992),
   ...
);


SELECT JSON_UNQUOTE(JSON_EXTRACT(data3, '$[0]')) AS result
FROM efr2;
["ADCBCCSSO"]
consider your annual leave plans for the upcoming year.
select JSON_UNQUOTE(JSON_EXTRACT(data2, '$."x-channel-id"')) from efr2;

JSON_UNQUOTE(JSON_EXTRACT(your_longtext_column, '$.`key-with-hyphen`')) AS extracted_value

[X-Request-ID:"d8e18241-ec69-49af-a986-7f1703574c15", X-Channel-ID:"5", x-fapi-interaction-id:"3ccaee28-0d1f-3898-adf4-c5c37ddd7a3e"]

{"authorization":["Basic ZGV2OmRldjEyMw=="],"x-channel-id":["ADCBCCSSO"],"x-fapi-interaction-id":["0002d15e-8609-3942-a1df-c58335e9d314"],"content-type":["application/json; charset=utf-8"],"content-length":["68"],"host":["10.101.166.114:9085"],"connectio...

JSON_UNQUOTE(JSON_EXTRACT(your_longtext_column, '$[?(@.key2)]')) AS extracted_value

SELECT *
FROM your_table
WHERE JSON_EXTRACT(your_longtext_column, '$.your_key') = 'your_value';


CALL FetchDataByDateRange('2024-01-01', '2024-01-31');

DELIMITER //

CREATE PROCEDURE FetchDataByDateRange(
    IN start_date DATE,
    IN end_date DATE
)
BEGIN
    WITH CTE_Table1 AS (
        SELECT 
            column1 AS t1_col1,
            column2 AS t1_col2,
            date_column AS t1_date
        FROM 
            table1
        WHERE 
            date_column BETWEEN start_date AND end_date
    ),
    
    CTE_Table2 AS (
        SELECT 
            column3 AS t2_col1,
            column4 AS t2_col2,
            date_column AS t2_date
        FROM 
            table2
        WHERE 
            date_column BETWEEN start_date AND end_date
    ),
    
    CombinedData AS (
        SELECT 
            t1.t1_col1,
            t1.t1_col2,
            t1.t1_date,
            t2.t2_col1,
            t2.t2_col2,
            t2.t2_date
        FROM 
            CTE_Table1 t1
        JOIN 
            CTE_Table2 t2 ON t1.t1_date = t2.t2_date
    )
    
    SELECT * FROM CombinedData;
    
END //

DELIMITER ;




DELIMITER //

CREATE PROCEDURE FetchDataByDateRange(
    IN start_date DATE,
    IN end_date DATE
)
BEGIN
    WITH CombinedData AS (
        SELECT 
            table1.*, 
            table2.*
        FROM 
            table1
        JOIN 
            table2 ON table1.common_column = table2.common_column  -- Replace with the actual common column
        WHERE 
            table1.date_column BETWEEN start_date AND end_date
            AND table2.date_column BETWEEN start_date AND end_date
    )
    
    SELECT * FROM CombinedData;
    
END //

DELIMITER ;
