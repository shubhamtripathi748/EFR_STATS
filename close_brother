# Oracle Database Connection Configuration
spring.datasource.url=jdbc:oracle:thin:@<host>:<port>:<service_name>
spring.datasource.username=<your-username>
spring.datasource.password=<your-password>
spring.datasource.driver-class-name=oracle.jdbc.OracleDriver

# JPA Configuration (optional if using JPA)
spring.jpa.hibernate.ddl-auto=update   # or "validate", "none"
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.Oracle12cDialect
spring.jpa.show-sql=true

# Connection Pool Configuration (optional, if using a connection pool like HikariCP)
spring.datasource.hikari.maximum-pool-size=10
spring.datasource.hikari.minimum-idle=5

# Other Properties for Connection Pooling
spring.datasource.tomcat.max-wait=10000
spring.datasource.tomcat.max-active=50
spring.datasource.tomcat.max-idle=20
spring.datasource.tomcat.min-idle=5


he Spring Boot application will fetch the image from S3 using the isi_barcode as a key and serve it to I-PROMPT through a dedicated REST API endpoint. This approach eliminates the need for I-PROMPT to include multiple if-else conditions for various scenarios, centralizing the logic within the Spring Boot application. This ensures:

Cleaner Code in I-PROMPT: By abstracting the logic into the Spring Boot application, I-PROMPT is only responsible for invoking the REST API and handling its response.
Flexibility: Any changes to the image-fetching logic (e.g., updating the S3 bucket structure) can be managed in one place without modifying I-PROMPT.
Scalability: The Spring Boot application can handle advanced logic like caching, retries, or load balancing, which is not feasible with static if-else conditions in I-PROMPT.
Better Separation of Concerns: Each system focuses on its core functionality, improving maintainability and reducing dependencies.

The Spring Boot application between I-Prompt and APIC serves as a middleware layer to handle communication, data transformation, validation, and error handling. By exposing a RESTful interface and utilizing services for APIC interaction, you can ensure a decoupled and manageable architecture for your system. Proper logging, security, and monitoring will ensure smooth operations and facilitate debugging during integration.

The delete operation should ideally not be part of the migration job for the following reasons:

It mixes responsibilities and can jeopardize data integrity.
It complicates recovery in case of migration failures.
It should be a separate task, where you can control access, logging, and retries, all of which are easier to manage through a dedicated REST API.
By decoupling migration and deletion tasks, you create a more modular, maintainable, and flexible architecture

Batch Size: Use chunk in Spring Batch to control batch processing size.
Failure Mechanism: Configure error handling with skip and retry strategies.
Retry Logic: Automatically retry failed items or steps based on exception types.
Scheduling: Use cron expressions in @Scheduled to automate job execution at fixed intervals.
Monitoring: Track job status and performance via metrics, logs, and monitoring tools.
By using Spring Batch’s features, you can manage batch processing efficiently, handle failures, and schedule jobs according to business requirements.

Modularity and Decoupling:
I-Prompt handles only file generation, while Spring Boot takes care of upload and status management.
Flexibility for Changes:
If the application evolves to include more complex workflows (e.g., retry logic or batch processing), a dedicated DB ensures smooth extensibilit

Ownership:
If Spring Boot fully owns the upload process, I-Prompt doesn't need to log statuses.
Separation of Concerns:
Mentioning I-Prompt Log DB creates unnecessary coupling and complexity if it isn't actively managing the upload workflow


A status helps track the progress of an operation. For instance, during the process of uploading files to AWS S3, the status can indicate:
"PENDING": The operation is queued but not yet started.
"IN_PROGRESS": The file is being uploaded.
"SUCCESS": The file has been successfully uploaded.
"FAILED": The upload encountered an error.
This tracking is crucial for understanding where in the process a particular file or operation currently stands

For Externalized Cron Expressions: The responsible team (DevOps or IT) can modify the schedule in the configuration file or through environment variables, followed by a restart or refresh of the application.

Dynamic Schedule Updates
For systems requiring frequent schedule changes without restarting, you can implement dynamic scheduling:
The cron expression can be changed dynamically in a database or external file.
Changes can be loaded without redeploying the application


=======================
When working with a team that uses a WSDL (Web Services Description Language) for retrieving data from a postcode vendor, you should ask questions to clarify the functionality, integration, performance, and constraints. Here’s a structured list of questions:

1. Purpose and Requirements

	•	What specific data are you retrieving from the postcode vendor (e.g., address details, geolocation, validation)?
	•	Why is the postcode data required, and how is it used within the system?
	•	Are there any compliance or data quality requirements for the postcode data?

2. Service Details

	•	What operations are provided by the WSDL (e.g., validation, lookup)?
	•	Is there documentation for the WSDL or the API it represents?
	•	What version of the WSDL is currently in use? Is it up-to-date?

3. Authentication and Security

	•	Does the WSDL require authentication (e.g., API keys, OAuth)?
	•	What security protocols are in place for communication (e.g., HTTPS, WS-Security)?
	•	Is the data from the vendor encrypted or sensitive? If so, how is it secured?

4. Integration and Implementation

	•	How is the WSDL integrated into your application (e.g., using a specific library, manually generating client code)?
	•	Are you using any frameworks (e.g., Spring-WS, Apache CXF) to consume the WSDL?
	•	Are there any dependencies or configurations required to connect to the service?

5. Performance and Reliability

	•	What is the expected latency and throughput of the service?
	•	Have there been any performance or availability issues with the postcode vendor?
	•	What is the SLA (Service Level Agreement) provided by the vendor?

6. Error Handling and Logging

	•	How are errors from the postcode service handled (e.g., invalid postcodes, network issues)?
	•	Is there a logging mechanism in place for tracking service calls and responses?
	•	Are retries implemented for transient errors? If yes, how are they configured?

7. Costs and Usage Limits

	•	Are there any rate limits or quotas for using the postcode service?
	•	What are the costs associated with using the postcode vendor?

8. Testing and Environment

	•	Is there a sandbox or test environment for the postcode service?
	•	How do you test the WSDL integration during development and deployment?

9. Vendor and Maintenance

	•	Who is the vendor, and how reliable is their service?
	•	How often does the vendor update the WSDL, and how are updates communicated to clients?
	•	What is the support mechanism for issues related to the postcode service?

10. Alternatives and Future Plans

	•	Are there plans to switch vendors or upgrade to a different service?
	•	Have alternative data sources or vendors been evaluated?

By addressing these areas, you ensure a clear understanding of the integration and




### Design Document: Handling AWS S3 Downtime and Connectivity Loss

---

#### **Overview**
This document outlines the handling strategy for two critical failure scenarios in a system integrated with AWS S3:
1. **AWS S3 Downtime**: When the S3 service is unavailable due to regional outages or internal failures.
2. **AWS Connectivity Loss**: When the application cannot connect to AWS services due to network issues, misconfigurations, or local server failures.

---

### **1. AWS S3 Downtime**

#### **Potential Issues**
- **Read Failures**: Unable to fetch data from S3 buckets.
- **Write Failures**: Unable to upload objects to S3.
- **Delete Failures**: Unable to delete objects from S3.

#### **Mitigation Strategies**
- **Retry Mechanism**:
  - Implement exponential backoff with a maximum retry limit for all S3 operations.
  - Example: Retry after 1 second, 2 seconds, 4 seconds, etc., up to a maximum of 5 retries.
  
- **Fallback Storage**:
  - Use a backup storage mechanism such as local storage or a relational database to temporarily hold data until S3 becomes available.
  - Example: Save images or files in a local folder or database with a status flag indicating pending S3 upload.

- **Alerting and Monitoring**:
  - Set up AWS CloudWatch alarms to detect S3 outages and notify the operations team.
  - Use application logs and monitoring tools like Prometheus and Grafana to track errors.

- **Data Recovery**:
  - For write failures, queue failed operations in a persistent storage (e.g., a database or message queue like Kafka) and retry once S3 is back online.
  - For read failures, serve pre-fetched or cached data to users.

---

### **2. AWS Connectivity Loss**

#### **Potential Issues**
- **All AWS Services Impacted**: This includes not just S3 but also other services like IAM, CloudWatch, etc.
- **Inconsistent Application Behavior**: If critical operations depend on S3, the entire system could fail.

#### **Mitigation Strategies**
- **Health Checks**:
  - Periodically check the connectivity to AWS endpoints using ping or status APIs.
  - Detect connectivity issues early and log errors for visibility.

- **Circuit Breaker Pattern**:
  - Use a circuit breaker pattern to prevent continuous retries when connectivity is lost.
  - Example: After several failed attempts, the circuit breaker opens, halting further requests temporarily.

- **Offline Queueing**:
  - Queue all operations requiring AWS connectivity in a persistent storage layer.
  - Example: Use a database or local file system to hold operations and process them once connectivity is restored.

- **Redundant Connectivity**:
  - Use multiple network routes or services like AWS Direct Connect and VPN to ensure redundancy.
  - Example: Switch between primary and backup internet connections when issues are detected.

- **Graceful Degradation**:
  - Offer limited functionality during connectivity issues.
  - Example: Allow users to perform non-S3-dependent tasks while queuing or deferring S3 operations.

---

### **Behavior Flow During Failures**

#### **Case: S3 Downtime**
1. Attempt S3 operation (upload, read, or delete).
2. If S3 is unavailable:
   - Retry the operation.
   - If retries fail:
     - Queue the operation for later processing.
     - Log the failure and send an alert.
     - Optionally store data in fallback storage.

3. Process queued operations once S3 is back online.

#### **Case: AWS Connectivity Loss**
1. Detect connectivity issues using health checks or monitoring tools.
2. Halt S3 operations temporarily (circuit breaker).
3. Queue all S3-related tasks in persistent storage.
4. Notify the operations team of the connectivity issue.
5. Resume normal operations when connectivity is restored.

---

### **Implementation Example**

#### **Retry Logic**
```java
int maxRetries = 5;
int retryCount = 0;
while (retryCount < maxRetries) {
    try {
        // Perform S3 operation
        s3Client.putObject(bucketName, key, file);
        break;
    } catch (SdkClientException e) {
        retryCount++;
        Thread.sleep((int) Math.pow(2, retryCount) * 1000); // Exponential backoff
    }
}
if (retryCount == maxRetries) {
    log.error("Failed to upload to S3 after multiple retries");
    queueOperationForRetry(bucketName, key, file);
}
```

#### **Circuit Breaker Example**
Use libraries like Resilience4j to implement circuit breaker patterns:
```java
CircuitBreakerConfig config = CircuitBreakerConfig.custom()
    .failureRateThreshold(50)
    .waitDurationInOpenState(Duration.ofSeconds(30))
    .build();
CircuitBreaker circuitBreaker = CircuitBreaker.of("s3Service", config);

Supplier<Response> decoratedCall = CircuitBreaker.decorateSupplier(circuitBreaker, () -> s3Service.uploadFile(file));
```

---

### **Summary of Recommendations**

| Failure Scenario       | Mitigation Strategy                             | Tools/Technologies                              |
|------------------------|------------------------------------------------|------------------------------------------------|
| **S3 Downtime**        | Retry mechanism, fallback storage, alerts      | Spring Retry, AWS SDK, CloudWatch              |
| **Connectivity Loss**  | Circuit breaker, offline queueing, redundancy  | Resilience4j, Kafka, AWS Direct Connect/VPN    |

By implementing these strategies, the system can handle S3 downtime or connectivity issues gracefully, ensuring data integrity and minimal disruption to users.
